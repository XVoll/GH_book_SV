[
["fit.html", "Section 5 Fit the model 5.1 Loading the data 5.2 Spatial dependency: neigbouring structure 5.3 Standardizing the data 5.4 Dimension reduction with Jacknife approach 5.5 Goodness of fit of the model on the test set 5.6 Literacy", " Section 5 Fit the model In this section we fit the model for income, literacy and poverty. This can be summarized in the following steps: Parametrization of the spatial dependence structure Standardization of the covariates Dimension reduction of set of covariates with the Jacknife approach Test of the goodness of fit of the model The covariates selection with the Jacknife approach is described, but the code is not run as it takes approximately 72 hours to complete. 5.1 Loading the data We start by loading the data and create a spatial polygon data frame. # rm(list=ls()) library(INLA) library(dplyr) library(leaflet) library(miscTools) dir_data=&quot;C:/Users/Xavier Vollenweider/Documents/Flowminder/IDB/ES_poverty_mapping/phase_II/data/&quot; # load the data #### # load the table ehpm17_predictors=read.csv(paste0(dir_data, &quot;out/ehpm17_predictors.csv&quot;)) # correct for xls missbehaviour: the SEG_ID with a leading 0 were shorten ehpm17_predictors=ehpm17_predictors%&gt;% mutate(SEG_ID=as.character(SEG_ID), SEG_ID=ifelse(nchar(SEG_ID)==7, paste0(0,SEG_ID), SEG_ID)) # shape SLV=rgdal::readOGR(paste0(dir_data, &quot;spatial/shape/admin/STPLAN_Departamentos.shp&quot;)) departamentos_sh=rgdal::readOGR(paste0(dir_data, &quot;spatial/shape/admin/STPLAN_Departamentos.shp&quot;)) segmento_sh=rgdal::readOGR(paste0(dir_data, &quot;spatial/shape/admin/STPLAN_Segmentos.shp&quot;)) # add the survey data to shapefile segmento_sh_data=segmento_sh segmento_sh_data@data=segmento_sh_data@data%&gt;% dplyr::select(SEG_ID)%&gt;% mutate(SEG_ID=as.character(SEG_ID))%&gt;% left_join(ehpm17_predictors, by=&quot;SEG_ID&quot;) 5.2 Spatial dependency: neigbouring structure In order to model the spatial dependency, we need to inform the model about which segmentos are neighbouring which ones. The function spdep::poly2nb creates a neighbours list from the segmento polygon. The list is created in a spatial format suitable for INLA with the function spdep::nb2INLA. It is then read as a graph-object for INLA consumption. # define the neighbouring structure #### segmento_sh_data@data$ID=1:length(segmento_sh_data@data$n_obs) segmento.nb=spdep::poly2nb(segmento_sh_data) spdep::nb2INLA(paste0(dir_data, &quot;out/SEG.graph&quot;), segmento.nb) SEG.adj=paste0(paste0(dir_data, &quot;out/SEG.graph&quot;), sep=&quot;&quot;) Segmento.Inla.nb &lt;- INLA::inla.read.graph(paste0(dir_data, &quot;out/SEG.graph&quot;)) 5.3 Standardizing the data We start by creating a series of binary variable identifying cantones with average income above 400. MPIO_CANTON_out=segmento_sh_data@data%&gt;% filter(segmento_sh_data@data$ingpe&gt;400)%&gt;% distinct(MPIO,CANTON)%&gt;% mutate(MPIO_CANTON=paste0(MPIO,CANTON)) segmento_sh_data@data=segmento_sh_data@data%&gt;% mutate(MPIO_CANTON=paste0(MPIO,CANTON), top_cantons=ifelse(MPIO_CANTON%in%MPIO_CANTON_out$MPIO_CANTON,&quot;yes&quot;,&quot;no&quot;), antiguo_urban=ifelse(MPIO_CANTON==&quot;ANTIGUO CUSCATLANAREA URBANA&quot;,&quot;yes&quot;,&quot;no&quot;), top_cantons=ifelse(MPIO_CANTON%in%MPIO_CANTON_out$MPIO_CANTON,&quot;yes&quot;,&quot;no&quot;), outliers=ifelse(ingpe&gt;quantile(ingpe,0.99,na.rm=T),&quot;out&quot;,&quot;no&quot;), top_outliers=ifelse(ingpe&gt;600,&quot;out&quot;,&quot;no&quot;)) We then select the segmentos where the EHPM survey data have been collected. # identify ehpm17 segmentos to fit the model #### segmento_sh_data_model=segmento_sh_data non_na_index=which(is.na(segmento_sh_data_model$n_obs)==F) ehpm17_predictors_nona=segmento_sh_data_model@data[non_na_index,] ehpm17_predictors_nona=ehpm17_predictors_nona%&gt;% mutate(settlements_c=cut(settlements, quantile(settlements,na.rm=T),include.lowest=T), slope_c=cut(slope, quantile(slope,na.rm=T),include.lowest=T)) The data exploration from previous section, allowed us to identify the following set of 18 candidate covariates to predict income: Table 5.1: Candidate covariates for income Candidates lights_med settlements lc_urban ndvi_17_median dist2urban_r ndvi_17_sum dist2allpubserv dist2roadInter Shape_Area dist2biz lc_tree gpw_es_TOT soil_deg_SDG_index dist2tree_r slope dens_all_roads dist2crop_r lc_crop In order to limit the effect of the presence of outliers in the covariates and of the various units in which the covariates are expressed, the covariates are standarized as folloows: \\[ \\begin{align} \\tilde{x}=\\frac{x_{i}-\\mu_{x}}{\\sigma_{x}} \\end{align} \\] where \\(x_{i}\\) is one of \\(i=1,...,c\\) covariates, \\(\\mu_{x}\\) is the sample mean of the covariate \\(x_{i}\\), \\(\\sigma_{x}\\) its standrd deviation and \\(\\tilde{x}\\) is the resulting standardized covariate. data_df=ehpm17_predictors_nona[,c(&quot;AREA_ID&quot;,&quot;SEG_ID&quot;,&quot;DEPTO&quot;,&quot;ID&quot;,&quot;settlements_c&quot;,&quot;slope_c&quot;,&quot;top_cantons&quot;, paste(cov_candidates_selected_df$var_income))] data_df$ID=as.character(data_df$ID) # turn ID to character to avoid having it standardized # covariates_or=data_df %&gt;% # mutate_if(is.character,as.factor) # # MyStd &lt;- function(x) {(x - mean(x)) / sd(x)} : apply it directly adj &lt;- caret::preProcess(data_df, method = c(&quot;center&quot;, &quot;scale&quot;), verbose = TRUE) adj covariates_sd &lt;- predict(adj, data_df) covariates_sd$intercept=1 covariates_sd$ID=as.numeric(covariates_sd$ID) length(covariates_sd)-2 Lastly, we store the covariates data into a dataframe along with the outcome variable data_model=data.frame(ingpe=ehpm17_predictors_nona$ingpe, covariates_sd) 5.4 Dimension reduction with Jacknife approach The aim is to decrease the number of covariates to less than 10. We will set this number quite arbitrarly to 8. The jackwise approach works as follows: Split the sample into a 60/20/20 training, validation and test sets Write the test formula without one of the \\(k=23\\) candidate covariates Fit the model on the training set Validate the model on the validation set and compute goodness of fit statistics Repeat step 2 to 4 once for each covariate (\\(k=23\\) times) Drop the covariate in the absence of which the RMSE is the lowest Repeat steps 2 to 6 until only 8 covariates are left 5.4.1 Split the sample into a 60/20/20 training, validation and test sets The model will be fitted on the training set. Income will be predicted on the validation. Prediction on the validation set will be compared with the observed datain order to identify which covariate to drop. Once the set of covariate is reduced to 8, the model is fitted on the training and validation set and prediction is made on the test set, a set of data to which the model has not been exposed. # sample segmentos for training, val and testing #### set.seed(102) index_train_val =sample(1:nrow(data_model), size=0.8*nrow(data_model)) # sample 80% of the data as train and validation sets index_val =sample(1:length(index_train_val), size=0.25*length(index_train_val)) # sample 0.25 of 0.8=0.2 as validation set mod_data_jack = data_model # store data into a new dataframe for model fitting mod_data_jack=mod_data_jack[index_train_val,] # select only the training and validation sets, keep test set of final validation mod_data_jack$pred=mod_data_jack$ingpe # create a pred variables equal to ingpe, the outcome variable mod_data_jack$pred[index_val] &lt;- NA # set the validation pred to NA: they will be predicted by the model and compared with observed values 5.4.2 Write the test formula without one of the \\(k=23\\) candidate covariates Before writing the formula, we make sure that the covariates expressed as factor are not presented twice cov_candidates_selected_df$var_income=as.character(cov_candidates_selected_df$var_income) # expressed_as_factors=which(cov_candidates_selected_df$var_income%in%c(&quot;settlements&quot;,&quot;slope&quot;)) covariates_formulation=c(&quot;factor(AREA_ID)&quot;, # &quot;factor(DEPTO)&quot;, &quot;factor(top_cantons)&quot;, cov_candidates_selected_df$var_income) We can then remove on covariate from the list and write the formula ix=1 # the index of the covariate to be removed candidate_covariates &lt;- covariates_formulation formula_test &lt;- reformulate(c(&quot;-1&quot;, &quot;intercept&quot;, paste(candidate_covariates[-ix],collapse=&quot;+&quot;), &#39;f(ID,model = &quot;besag&quot;, graph = Segmento.Inla.nb, scale.model = TRUE)&#39;), &quot;pred&quot;) formula_test ## pred ~ -1 + intercept + factor(top_cantons) + lights_med + settlements + ## lc_urban + ndvi_17_median + dist2urban_r + ndvi_17_sum + ## dist2allpubserv + dist2roadInter + Shape_Area + dist2biz + ## lc_tree + gpw_es_TOT + soil_deg_SDG_index + dist2tree_r + ## slope + dens_all_roads + dist2crop_r + lc_crop + f(ID, model = &quot;besag&quot;, ## graph = Segmento.Inla.nb, scale.model = TRUE) 5.4.3 Fit the model on the training set The model is fitted with the inla command. The formula argument has been defined above. The family argument defines the type of likelihood function for the response variable. As the distribution of income is strickly positive and right skewed, a gamma distribution is chosen. The control.compute provide a way to select which goodness of fit measures are computed. We turn the CPO to FALSE to speed up the computation. Lastly, we adopt a simplified integration stratgey through the command list(int.strategy = &quot;eb&quot;). start_time=Sys.time() # time the start of the estimation besag.res=inla(formula=formula_test, family = &quot;gamma&quot;, data = mod_data_jack, control.compute=list(dic=T, cpo=F), control.inla =list(int.strategy = &quot;eb&quot;) ) end_time=Sys.time() # time at the end of the estimation duration=end_time-start_time print(duration) ## Time difference of 32.84653 secs 5.4.4 Validate the model on the validation set and compute goodness of fit statistics We can now investigate the goodness of fit of the model. We start by extracting the average predicted value. M_test=besag.res$summary.fitted.values[,&quot;mean&quot;] The prediction for the validation set need to be rescaled: M_test[index_val]=exp(M_test[index_val]) The observed values are plotted against the predicted value for the validation set and the r-squared and the root mean squared errors are computed, two goodness of fit statistics. res &lt;- mod_data_jack$ingpe[index_val]-M_test[index_val] RMSE_test &lt;- sqrt(mean(res^2,na.rm=T)) res &lt;- mod_data_jack$ingpe[-index_val]-M_test[-index_val] RMSE_train &lt;- sqrt(mean(res^2,na.rm=T)) r2_test=cor(mod_data_jack$ingpe[index_val],M_test[index_val])^2 r2_train=cor(mod_data_jack$ingpe[-index_val],M_test[-index_val])^2 a &lt;- list( x = 600, y = 100, text = paste(&quot;R2 training set:&quot;,round(r2_train*100),&quot;%&quot;, &quot;\\nR2 validation set:&quot;,round(r2_test*100),&quot;%&quot;, &quot;\\n&quot;, &quot;\\nRMSE training set:&quot;,round(RMSE_train),&quot;USD&quot;, &quot;\\nRMSE validation set:&quot;,round(RMSE_test),&quot;USD&quot;), xref = &quot;x&quot;, yref = &quot;y&quot;, showarrow = F) plotly::plot_ly(y=M_test[-index_val], x=mod_data_jack$ingpe[-index_val], type=&quot;scatter&quot;, mode=&quot;markers&quot;, name=&quot;training set&quot;, marker = list(color = &#39;black&#39;, opacity = 0.3))%&gt;% plotly::add_trace(y=M_test[index_val], x=mod_data_jack$ingpe[index_val], mode=&quot;markers&quot;, name=&quot;validation set&quot;, marker = list(color = &#39;red&#39;, opacity = 0.3))%&gt;% plotly::add_trace(y=c(0,880), x=c(0,880), mode=&quot;lines&quot;, name=&quot;1:1&quot;)%&gt;% plotly::layout(yaxis=list(range=c(0,510),title=&quot;predicted (USD)&quot;), xaxis=list(range=c(0,880),title=&quot;observed (USD)&quot;), annotations=a, title=&quot;Income at the segmento level&quot;) The model appears to do relatively good job, although it has difficulty to capture the segmentos with an income above 400 despite the inclusion of the canton binary variable for canton with average income above 400. An option would be to control for these segmentos explicitly. However, we do not have a list of segmentos with income above 400 which include also the segmentos were the EHPM survey was not implemented. The RMSE is 48 usd in the validation set while the the r-squared is 46%, so relatively good. 5.4.5 Repeat step 2 to 4 once for each covariate We repeat steps 2 to 4 for each covariate, i.e. estimate the model 23 times. It took 29 second to estimate one model, hence we can expect to need circa 11 minutes to go through the 23 covariates. In order to ease the reading of the code, we create a function INLA_steps_2_4 implementing steps 2 to 4 of the covariate selection process. INLA_steps_2_4=function(covariates_formulation, # formulation of each covariate ix, # the index of the covariate to be removed index_val, # index of the validation set mod_data_jack, # data for the model outcome, # string defining the outcome variable, e.g. ingpe family){ # string defining the likelihood # formula candidate_covariates &lt;- covariates_formulation formula_test &lt;- reformulate(c(&quot;-1&quot;, &quot;intercept&quot;, paste(candidate_covariates[-ix],collapse=&quot;+&quot;), &#39;f(ID,model = &quot;besag&quot;, graph = Segmento.Inla.nb, scale.model = TRUE)&#39;), &quot;pred&quot;) # fit the model if(family%in%c(&quot;beta&quot;,&quot;zeroinflatedbinomial1&quot;)){ besag.res=inla(formula=formula_test, family = family, data = mod_data_jack, Ntrials = mod_data_jack$n_obs, control.compute=list(dic=T, cpo=F), control.inla =list(int.strategy = &quot;eb&quot;) ) }else{ besag.res=inla(formula=formula_test, family = family, data = mod_data_jack, control.compute=list(dic=T, cpo=F), control.inla =list(int.strategy = &quot;eb&quot;) ) } # extact fitted values M_test=besag.res$summary.fitted.values[,&quot;mean&quot;] if(family==&quot;gamma&quot;){ M_test[index_val]=exp(M_test[index_val]) } if(family%in%c(&quot;beta&quot;,&quot;zeroinflatedbinomial1&quot;)){ M_test[index_val]=exp(M_test[index_val])/(1+exp(M_test[index_val])) } # RMSE res &lt;- mod_data_jack[index_val,outcome]-M_test[index_val] RMSE_val &lt;- sqrt(mean(res^2,na.rm=T)) res &lt;- mod_data_jack[-index_val,outcome]-M_test[-index_val] RMSE_train &lt;- sqrt(mean(res^2,na.rm=T)) # R2 r2_val=cor(mod_data_jack[index_val,outcome],M_test[index_val])^2 r2_train=cor(mod_data_jack[-index_val,outcome],M_test[-index_val])^2 # store results results_list=list(&quot;cov_i&quot;=ix, &quot;cov_name&quot;=candidate_covariates[ix], &quot;formula&quot;=formula_test, &quot;fitted_values&quot;=M_test, &quot;index_val&quot;=index_val, &quot;index_train_val&quot;=index_train_val, &quot;RMSE_val&quot;=RMSE_val, &quot;RMSE_train&quot;=RMSE_train, &quot;r2_val&quot;=r2_val, &quot;r2_train&quot;=r2_train, &quot;outcome&quot;=outcome, &quot;family&quot;=family) return(results_list) } The function INLA_steps_2_4 is used to loop over the covariates as follows: results_list=list() for(cov_n in 1:length(covariates_formulation)){ model_fct=INLA_steps_2_4(covariates_formulation, # formulation of each covariate cov_n, # the index of the covariate to be removed index_val, # index of the validation set mod_data_jack, # data for the model &quot;ingpe&quot;, # outcome &quot;gamma&quot;) # likelihood results_list[[cov_n]]=model_fct cat(&quot;\\014&quot;) print(cov_n) } 5.4.6 Drop the covariate in the absence of which the RMSE is the lowest The model with lowest RMSE is identified in order to drop the corresponding covariate, i.e. the covariate the removal of which affects the least the model performance. rmse_val=lapply(results_list,function(x) unlist(x$RMSE_val)) rmse_val_min=which.min(rmse_val) covariates_to_be_removed=results_list[[rmse_val_min]]$cov_name candidate_covariates=candidate_covariates[-which(candidate_covariates==covariates_to_be_removed)] 5.4.7 Repeat steps 2 to 6 until only 8 covariates are left We wrap up step 2 to 6 in one loop. candidate_covariates=covariates_formulation # start again with the full list of candidate covariate n2drop=length(covariates_formulation)-8 # 8 is the abritrary number of covariates we want to keep start_t=Sys.time() results_jacknife=list() # create a list to store all the results of the jacknife selectio process for(n_cov_to_drop in 1:n2drop){ # repeat the step n2drop=12 times until having only 8 covariates set.seed(101) results_list=list() for(cov_n in 1:length(candidate_covariates)){ # fit the model dropping one covariates after the other model_fct=INLA_steps_2_4(candidate_covariates, # formulation of each covariate cov_n, # the index of the covariate to be removed index_val, # index of the validation set mod_data_jack, # data for the model &quot;ingpe&quot;, # outcome &quot;gamma&quot;) # likelihood results_list[[cov_n]]=model_fct # store the results cat(&quot;\\014&quot;) # clean the consol print(paste(&quot;loop&quot;,n_cov_to_drop, &quot;of&quot;, n2drop, &quot;cov to drop;&quot;, &quot;cov&quot;,cov_n, &quot;r2&quot;,model_fct$r2_val)) end_t=Sys.time() duration=end_t-start_t print(duration) } # store all the results for quality check results_jacknife[[n_cov_to_drop]]=results_list # drop covariate affecting the least the goodness of fit rmse_val=lapply(results_list,function(x) unlist(x$RMSE_val)) rmse_val_min=which.min(rmse_val) covariates_to_be_removed=results_list[[rmse_val_min]]$cov_name candidate_covariates=candidate_covariates[-which(candidate_covariates==covariates_to_be_removed)] print(paste(length(candidate_covariates),&quot;covariates remaining&quot;)) } end_t=Sys.time() duration=end_t-start_t print(duration) # 1.46 hours covaritates_selected=candidate_covariates covaritates_selected The loop took circa one hour thirty to complete. The preferred specification is: # formula formula_selected = reformulate(c(&quot;-1&quot;, &quot;intercept&quot;, paste(covaritates_selected,collapse=&quot;+&quot;), &#39;f(ID,model = &quot;besag&quot;, graph = Segmento.Inla.nb, scale.model = TRUE)&#39;), &quot;pred&quot;) ## pred ~ -1 + intercept + factor(AREA_ID) + lights_med + settlements + ## dist2roadInter + Shape_Area + lc_tree + soil_deg_SDG_index + ## dist2tree_r + f(ID, model = &quot;besag&quot;, graph = Segmento.Inla.nb, ## scale.model = TRUE) 5.5 Goodness of fit of the model on the test set Finally, we validate the final model on the test set, i.e. the 20% of the sample that was not used to select the covariates. 5.5.1 Segmento level The model is fitted on the combined training and validation sets and prediction are done for the test set. # build dataframe for test data_test=data_model data_test$pred=data_test$ingpe # create a pred variables equal to ingpe, the outcome variable data_test$pred[-index_train_val] &lt;- NA # set the test pred to NA: they will be predicted by the model and compared with observed # fit the model besag_income=inla(formula=formula_selected, family = &quot;gamma&quot;, data = data_test, control.compute=list(dic=T, cpo=F), control.predictor =list(link=1, compute=1), control.inla =list(int.strategy = &quot;eb&quot;) ) We can now investigate the goodness of the model on the test set. 5.5.2 Municipio level The EHPM is representative at the municipio level for a subset of the mucipios. We can hence compare the goodness of fit of the model in representative and non representive municipios. First we compute the average income level on the entire sample at the municipio level. We then compute the average prediction of the test set at the municipio level. Finally, we compare observed average income agains predicted average income at the municipio level, both for the representative and non-representative municipios. At the municipio level, the R-squared is 73% and the RMSE drops to 23 USD. Focusing only on municipios where the EHPM data are representative, the R-squared increases to 84% while it drops to 33% for non-representative municipios. 5.6 Literacy The same process is adopted for literacy. The only difference here is the use of a Beta distribution for the likelihood function instead of the Gaussian and a different set of covariates used as candidate covariates. 5.6.1 Model fitting The selected formula is given by: formula_lit=pred ~ -1 + intercept + factor(DEPTO)+dist2biz+lc_tree+ dist2road+dens_all_roads+chirps_ev_sd+dist2tree_r+temp_median + f(ID, model = &quot;besag&quot;, graph = Segmento.Inla.nb, scale.model = TRUE) formula_lit ## pred ~ -1 + intercept + factor(DEPTO) + dist2biz + lc_tree + ## dist2road + dens_all_roads + chirps_ev_sd + dist2tree_r + ## temp_median + f(ID, model = &quot;besag&quot;, graph = Segmento.Inla.nb, ## scale.model = TRUE) And the model is fitted on the test set by: data_model=data.frame(illiteracy_rate=1-ehpm17_predictors_nona$literacy_rate, covariates_sd) data_model$illiteracy_rate=data_model$illiteracy_rate+1/10000 data_test=data_model data_test$pred=data_test$illiteracy_rate # create a pred variables equal to illiteracy_rate data_test$pred[-index_train_val] &lt;- NA # set the test pred to NA: they will be predicted by the model and compared with observed # fit the model besag_lit=inla(formula=formula_selected, family = &quot;beta&quot;, data = data_test, control.compute=list(dic=T, cpo=F), control.predictor = list(compute=T,link=1), control.inla =list(int.strategy = &quot;eb&quot;) ) 5.6.2 Goodness of fit at the segmento level We can now investigate the goodness of the model on the test set. 5.6.3 Goodness of fit at the municipio level And, following the same process than income to aggregate results at the municipio level, the goodness of fit is given by: The difference between representative and non-representative municipios is much less pronounced than in the case of income: the R-squared is 77% and 63% representative and non-representative municipios, respectively and the overall R-squared is 70%. In the next section, we investigate the effect of covariates and derive the maps. "]
]
